{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "788c7a583b49e0f108948e9844ef083ea97a54690da4d8e91da56fd70fa16a57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import gensim.models.doc2vec\n",
    "from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "from random import randint \n",
    "from itertools import count\n",
    "import networkx as nx\n",
    "import csv\n",
    "from csv import reader\n",
    "import sys\n",
    "import csv\n",
    "csv.field_size_limit(256<<10)\n",
    "csv.field_size_limit()\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Black_keywords = ['man', 'woman', 'men', 'women', 'male', 'female', 'person', 'people', 'community', 'neighborhood', 'child', 'children', 'kid', 'youth', 'business', 'company']\n",
    "\n",
    "Black_ethnicities = ['black', 'cape verdean', 'afro latino', 'afro latina', 'haitian', 'african american', 'african-american', 'caribbean', 'jamaican', 'dominican', 'west indian']\n",
    "\n",
    "#pick words from Black_keywords and Black_ethnicities\n",
    "keywords = ['man', 'woman', 'men', 'women', 'male', 'female', 'person', 'people', 'community', \n",
    "            'neighborhood', 'child', 'children','kid', 'youth', 'business', 'company', 'black', \n",
    "            'haitian', 'caribbean', 'jamaican', 'dominican']\n",
    "\n",
    "\n",
    "Neighbor_words = ['Brighton', 'Allston', 'Fenway', 'Longwood Medical Area', 'Back Bay', 'Beacon Hill', 'West End', 'North End', 'Downtown', 'Charlestown', 'East Boston', 'South Boston', 'South Boston Waterfront', 'South End', 'Roxbury', 'Mission Hill', 'Jamaica Plain', 'Dorchester', 'Mattapan', 'Roslindale', 'West Roxbury','Hyde Park','Harbor Islands']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters from bostonglobe 2014 flie\n",
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "            \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "            \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
    "            \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\", \"\\xc2\", \"\\xa0\",\n",
    "            \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \"\\xad\", \"\\xe2\", \"\\x9d\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2073\n"
     ]
    }
   ],
   "source": [
    "#bostonglobe2014-----------------------\n",
    "lst = []\n",
    "ifile  = open('./data-source/bostonglobe2014.csv', \"rb\")\n",
    "read = csv.reader(codecs.iterdecode(ifile, 'utf-8'))\n",
    "for row in read :\n",
    "    lst = lst + row \n",
    "print(len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<filter at 0x20bbe512340>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "filter(lambda k: 'Brighton' in k, lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up doc2vec model\n",
    "# tokenize the data and clean it using the simple_preprocess method\n",
    "def tokenize(text, stopwords, max_len = 20):\n",
    "    return [token for token in gensim.utils.simple_preprocess(text, max_len=max_len) if token not in stopwords]\n",
    "\n",
    "# convert the tokenized data into one big list of tokens, as opposed to a list of tokenized articles\n",
    "def target_doc (df):\n",
    "    articles = df.values.tolist()\n",
    "    articles_flat = [item for sublist in articles for item in sublist]\n",
    "    tagged_docs = [gensim.models.doc2vec.TaggedDocument(tokenize(text, [], max_len=200), [i]) for i, text in enumerate(articles_flat)]\n",
    "    return tagged_docs\n",
    "    \n",
    "# create the Doc2Vec model, build the model vocabulary, and train the model on the data\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=30, epochs=40, window=2, dm=1)\n",
    "model.build_vocab(target_doc (df_14))\n",
    "model.train(target_doc (df_14), total_examples=model.corpus_count, epochs=model.epochs)\n",
    "vector = model.infer_vector(Black_ethnicities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding similar words and creating a csv file\n",
    "\n",
    "def compute_similar_words(model,source_word, topn=5):\n",
    "    similar_words = [source_word]\n",
    "    try:\n",
    "        top_words = model.wv.most_similar(source_word,topn=topn)\n",
    "        similar_words.extend([val[0] for val in top_words])\n",
    "    except KeyError as err:\n",
    "        print(err.args)\n",
    "    return similar_words    \n",
    "\n",
    "def compute_similar_words_for_all_tasks(model,topn=5):\n",
    "    columns = ['word'+str(i-1) for i in range(1,topn+2)]\n",
    "    df = pd.DataFrame(data=None,columns=columns)\n",
    "    for source_word in keywords:\n",
    "        similar_words = compute_similar_words(model,source_word,topn)\n",
    "        df.loc[len(df)] = similar_words\n",
    "    for i in range(1,len(similar_words)):\n",
    "        sec_similar_words = compute_similar_words(model,similar_words[i],topn)\n",
    "        df.loc[len(df)] = sec_similar_words\n",
    "    df.to_csv('./similar_words_task/similar_words_task_2014.csv')\n",
    "    # df.to_csv('./similar_words_task/similar_words_task_2015.csv')\n",
    "    # df.to_csv('./similar_words_task/similar_words_task_2016.csv')\n",
    "    # df.to_csv('./similar_words_task/similar_words_task_2017.csv')\n",
    "    # df.to_csv('./similar_words_task/similar_words_task_2018.csv')\n",
    "\n",
    "words = compute_similar_words_for_all_tasks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in similar words csv file to create WordClouds\n",
    "\n",
    "words = pd.read_csv('./similar_words_task/similar_words_task_2014.csv')\n",
    "# words = pd.read_csv('./similar_words_task/similar_words_task_2015.csv')\n",
    "# words = pd.read_csv('./similar_words_task/similar_words_task_2016.csv')\n",
    "# words = pd.read_csv('./similar_words_task/similar_words_task_2017.csv')\n",
    "# words = pd.read_csv('./similar_words_task/similar_words_task_2018.csv')\n",
    "\n",
    "G = nx.Graph()\n",
    "j = 1\n",
    "for i, row in words.iterrows():\n",
    "    for j in range(1,len(row)):\n",
    "        G.add_node(i,label=row[j])\n",
    "    for j in range(1,len(row)):\n",
    "        G.add_edge(row[1], row[j])\n",
    "remove = [node for node,degree in dict(G.degree()).items() if degree > 2]\n",
    "# print(remove)\n",
    "pos = nx.spring_layout(G,k=0.3)\n",
    "betCent = nx.betweenness_centrality(G, normalized=True, endpoints=True)\n",
    "node_color = [20000.0 * G.degree(v) for v in G]\n",
    "node_size =  [v * 10000 for v in betCent.values()]\n",
    "plt.figure(figsize=(20,20))\n",
    "nx.draw_networkx(G, pos=pos, with_labels=True,\n",
    "                 node_color=node_color,\n",
    "                 node_size=node_size )\n",
    "plt.axis('off')\n",
    "plt.savefig(\"../img/network_graph_2014.png\")\n",
    "plt.show()"
   ]
  }
 ]
}