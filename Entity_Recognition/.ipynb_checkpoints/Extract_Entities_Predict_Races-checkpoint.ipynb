{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have (article text, article id) tuples for each neighborhood\n",
    "# turn all articles into spacy-processible list of documents, but maintain the tuples\n",
    "    # the result would be a tuple of a list of documents and the article id\n",
    "# for each list, extract out all people, feed their last names to ethnicolr to predict races and then tag the result with the article id\n",
    "# for each neighborhood, get percentage of races\n",
    "# journalism team will compare results with U.S. Census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import en_core_web_md\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medium English model in case we need to work with vectors\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Neighborhood_Separated_Articles/2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_neighborhoods = ['dorchester', 'roxbury', 'mattapan', 'hyde_park']\n",
    "white_neighborhoods = ['fenway', 'beacon_hill', 'downtown', 'south_boston', 'east_boston', 'back_bay', 'jamaica_plain',\n",
    "                      'south_end', 'charlestown', 'brighton', 'allston', 'west_end', 'roslindale', 'north_end',\n",
    "                      'mission_hill', 'harbor_islands', 'longwood_medical_area', 'west_roxbury']\n",
    "df = df.fillna(\"('no article', 'no_id')\")\n",
    "df['dorchester'] = df['dorchester'].apply(ast.literal_eval)\n",
    "df['roxbury'] = df['roxbury'].apply(ast.literal_eval)\n",
    "df['mattapan'] = df['mattapan'].apply(ast.literal_eval)\n",
    "df['hyde_park'] = df['hyde_park'].apply(ast.literal_eval)\n",
    "df['fenway'] = df['fenway'].apply(ast.literal_eval)\n",
    "df['beacon_hill'] = df['beacon_hill'].apply(ast.literal_eval)\n",
    "df['downtown'] = df['downtown'].apply(ast.literal_eval)\n",
    "df['south_boston'] = df['south_boston'].apply(ast.literal_eval)\n",
    "df['east_boston'] = df['east_boston'].apply(ast.literal_eval)\n",
    "df['back_bay'] = df['back_bay'].apply(ast.literal_eval)\n",
    "df['jamaica_plain'] = df['jamaica_plain'].apply(ast.literal_eval)\n",
    "df['south_end'] = df['south_end'].apply(ast.literal_eval)\n",
    "df['charlestown'] = df['charlestown'].apply(ast.literal_eval)\n",
    "df['brighton'] = df['brighton'].apply(ast.literal_eval)\n",
    "df['allston'] = df['allston'].apply(ast.literal_eval)\n",
    "df['west_end'] = df['west_end'].apply(ast.literal_eval)\n",
    "df['roslindale'] = df['roslindale'].apply(ast.literal_eval)\n",
    "df['north_end'] = df['north_end'].apply(ast.literal_eval)\n",
    "df['mission_hill'] = df['mission_hill'].apply(ast.literal_eval)\n",
    "df['harbor_islands'] = df['harbor_islands'].apply(ast.literal_eval)\n",
    "df['longwood_medical_area'] = df['longwood_medical_area'].apply(ast.literal_eval)\n",
    "df['west_roxbury'] = df['west_roxbury'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                  \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                  \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"–\", \n",
    "                  \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                  \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\"]\n",
    "\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "#for char in spec_chars:\n",
    "#    df['text'] = df['text'].str.strip()\n",
    "#    df['text'] = df['text'].str.replace(char, ' ')\n",
    "       \n",
    "# access each column separately\n",
    "for i in range(len(df.index)):\n",
    "    for col in df.columns:\n",
    "        for char in spec_chars:\n",
    "            try:\n",
    "                df.loc[i, col][0] = df.loc[i, col][0].str.strip()\n",
    "                df.loc[i, col][0] = df.loc[i, col][0].str.replace(char, ' ')\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 22)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyde_park DONE\n",
      "beacon_hill DONE\n",
      "south_boston DONE\n",
      "jamaica_plain DONE\n",
      "east_boston DONE\n",
      "south_end DONE\n",
      "back_bay DONE\n",
      "north_end DONE\n",
      "west_roxbury DONE\n",
      "mission_hill DONE\n",
      "harbor_islands DONE\n",
      "west_end DONE\n",
      "dorchester DONE\n",
      "roxbury DONE\n",
      "downtown DONE\n",
      "fenway DONE\n",
      "mattapan DONE\n",
      "brighton DONE\n",
      "charlestown DONE\n",
      "roslindale DONE\n",
      "allston DONE\n"
     ]
    }
   ],
   "source": [
    "articles = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], #'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "for sub_neighborhood in articles.keys():\n",
    "    for i in range(df.shape[0]):\n",
    "        if type(df.loc[i, sub_neighborhood]) == tuple:\n",
    "            articles[sub_neighborhood].append((nlp(df.loc[i, sub_neighborhood][0]), df.loc[i, sub_neighborhood][1]))\n",
    "    print(sub_neighborhood + ' DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014_5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['dorchester'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [],# 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "\n",
    "for sub_neighborhood in articles.keys():\n",
    "    for (doc, article_id) in articles[sub_neighborhood]:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                name = ent[0:2]\n",
    "                sentence = ent.sent\n",
    "                people[sub_neighborhood].append((name, sentence, article_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_neighborhood in people.keys():\n",
    "    list1 = people[sub_neighborhood]\n",
    "    # insert the list to the set\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    people[sub_neighborhood] = unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_proportions = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [],# 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "for sub_neighborhood in people.keys():\n",
    "    for i in range(len(people[sub_neighborhood])):\n",
    "        if people[sub_neighborhood][i][0].text.strip() != '':\n",
    "            temp = people[sub_neighborhood][i][0].text.split()\n",
    "            if len(temp) > 1:\n",
    "                people[sub_neighborhood][i] = (temp[-1], people[sub_neighborhood][i][1], people[sub_neighborhood][i][2])\n",
    "            else:\n",
    "                people[sub_neighborhood][i] = (temp[0], people[sub_neighborhood][i][1], people[sub_neighborhood][i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Darriean',\n",
       " “We were always horrified that child protective services were going to come and ship us off ”As the chaos unfolded around her  little Darriean’s natural bounciness gave way  She would become stone quiet and impassive  as though  one law enforcement official said  she were shutting down Court records don’t show whether it was Cheri’s decision or the state’s to send 8 year old Darriean away  “I need time to obtain medical care and help I need to recover ” Hess wrote in court papers in 2002 granting temporary custody of Darriean to her aunt and uncle in Alaska  ,\n",
       " '2014_90')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people['dorchester'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethnicolr import census_ln, pred_census_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = pd.DataFrame(people[white_neighborhoods[2]], columns=['last_name', 'article', 'article_id'])\n",
    "#temp1 = pd.DataFrame(people[white_neighborhoods[3]], columns=['last_name', 'article', 'article_id'])\n",
    "#pd.concat([pred_census_ln(temp, 'last_name', 2010), pred_census_ln(temp1, 'last_name', 2010)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fenway DONE\n",
      "beacon_hill DONE\n",
      "downtown DONE\n",
      "south_boston DONE\n",
      "east_boston DONE\n",
      "back_bay DONE\n",
      "jamaica_plain DONE\n",
      "south_end DONE\n",
      "charlestown DONE\n",
      "brighton DONE\n",
      "allston DONE\n",
      "west_end DONE\n",
      "roslindale DONE\n",
      "north_end DONE\n",
      "mission_hill DONE\n",
      "harbor_islands DONE\n",
      "west_roxbury DONE\n",
      "dorchester DONE\n",
      "roxbury DONE\n",
      "mattapan DONE\n",
      "hyde_park DONE\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame(columns=['last_name', 'article', 'article_id'])\n",
    "sub_neighborhoods = white_neighborhoods + black_neighborhoods\n",
    "sub_neighborhoods.remove('longwood_medical_area')\n",
    "for col in sub_neighborhoods:\n",
    "    temp = pd.DataFrame(people[col], columns=['last_name', 'article', 'article_id'])\n",
    "    preds = pred_census_ln(temp, 'last_name', 2010)\n",
    "    final_df = pd.concat([final_df, preds], axis=0)\n",
    "    print(col + ' DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('People_Covered_the_News/people_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If name has 'word', 'word', then take the first name\n",
    "# keep sentence where name occurred, okay if multiple sentences\n",
    "# look at sentence where the name was mentioned \n",
    "# and the words which were used\n",
    "# end up with a dataset which has 'name' + 'sentence' + 'race'\n",
    "# try to put ID of article in the dataset as well, next to the sentence\n",
    "# for now, try to keep the row from which the name comes, or at least some form of ID\n",
    "\n",
    "# if extra time, group sentences by associated race\n",
    "# find most frequently used words for each race, maybe a word cloud or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-organize the data so that we have a way to retrieve original text\n",
    "# like adding ID to the dataset to identify each article\n",
    "# we should be able to find out the article a name comes from\n",
    "# we should also be able to find out which neighborhood an article talks about"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
