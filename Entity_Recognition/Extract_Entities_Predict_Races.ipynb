{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have (article text, article id) tuples for each neighborhood\n",
    "# turn all articles into spacy-processible list of documents, but maintain the tuples\n",
    "    # the result would be a tuple of a list of documents and the article id\n",
    "# for each list, extract out all people, feed their last names to ethnicolr to predict races and then tag the result with the article id\n",
    "# for each neighborhood, get percentage of races\n",
    "# journalism team will compare results with U.S. Census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import en_core_web_md\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medium English model in case we need to work with vectors\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Neighborhood_Separated_Articles/2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_neighborhoods = ['dorchester', 'roxbury', 'mattapan', 'hyde_park']\n",
    "white_neighborhoods = ['fenway', 'beacon_hill', 'downtown', 'south_boston', 'east_boston', 'back_bay', 'jamaica_plain',\n",
    "                      'south_end', 'charlestown', 'brighton', 'allston', 'west_end', 'roslindale', 'north_end',\n",
    "                      'mission_hill', 'harbor_islands', 'longwood_medical_area', 'west_roxbury']\n",
    "df = df.fillna(\"('no article', 'no_id')\")\n",
    "df['dorchester'] = df['dorchester'].apply(ast.literal_eval)\n",
    "df['roxbury'] = df['roxbury'].apply(ast.literal_eval)\n",
    "df['mattapan'] = df['mattapan'].apply(ast.literal_eval)\n",
    "df['hyde_park'] = df['hyde_park'].apply(ast.literal_eval)\n",
    "df['fenway'] = df['fenway'].apply(ast.literal_eval)\n",
    "df['beacon_hill'] = df['beacon_hill'].apply(ast.literal_eval)\n",
    "df['downtown'] = df['downtown'].apply(ast.literal_eval)\n",
    "df['south_boston'] = df['south_boston'].apply(ast.literal_eval)\n",
    "df['east_boston'] = df['east_boston'].apply(ast.literal_eval)\n",
    "df['back_bay'] = df['back_bay'].apply(ast.literal_eval)\n",
    "df['jamaica_plain'] = df['jamaica_plain'].apply(ast.literal_eval)\n",
    "df['south_end'] = df['south_end'].apply(ast.literal_eval)\n",
    "df['charlestown'] = df['charlestown'].apply(ast.literal_eval)\n",
    "df['brighton'] = df['brighton'].apply(ast.literal_eval)\n",
    "df['allston'] = df['allston'].apply(ast.literal_eval)\n",
    "df['west_end'] = df['west_end'].apply(ast.literal_eval)\n",
    "df['roslindale'] = df['roslindale'].apply(ast.literal_eval)\n",
    "df['north_end'] = df['north_end'].apply(ast.literal_eval)\n",
    "df['mission_hill'] = df['mission_hill'].apply(ast.literal_eval)\n",
    "df['harbor_islands'] = df['harbor_islands'].apply(ast.literal_eval)\n",
    "df['longwood_medical_area'] = df['longwood_medical_area'].apply(ast.literal_eval)\n",
    "df['west_roxbury'] = df['west_roxbury'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                  \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                  \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"–\", \n",
    "                  \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                  \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\"]\n",
    "\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "#for char in spec_chars:\n",
    "#    df['text'] = df['text'].str.strip()\n",
    "#    df['text'] = df['text'].str.replace(char, ' ')\n",
    "       \n",
    "# access each column separately\n",
    "for i in range(len(df.index)):\n",
    "    for col in df.columns:\n",
    "        for char in spec_chars:\n",
    "            try:\n",
    "                df.loc[i, col][0] = df.loc[i, col][0].str.strip()\n",
    "                df.loc[i, col][0] = df.loc[i, col][0].str.replace(char, ' ')\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 22)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyde_park DONE\n",
      "beacon_hill DONE\n",
      "south_boston DONE\n",
      "jamaica_plain DONE\n",
      "east_boston DONE\n",
      "south_end DONE\n",
      "back_bay DONE\n",
      "north_end DONE\n",
      "west_roxbury DONE\n",
      "mission_hill DONE\n",
      "harbor_islands DONE\n",
      "west_end DONE\n",
      "longwood_medical_area DONE\n",
      "dorchester DONE\n",
      "roxbury DONE\n",
      "downtown DONE\n",
      "fenway DONE\n",
      "mattapan DONE\n",
      "brighton DONE\n",
      "charlestown DONE\n",
      "roslindale DONE\n",
      "allston DONE\n"
     ]
    }
   ],
   "source": [
    "articles = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "for sub_neighborhood in articles.keys():\n",
    "    for i in range(df.shape[0]):\n",
    "        if type(df.loc[i, sub_neighborhood]) == tuple:\n",
    "            articles[sub_neighborhood].append((nlp(df.loc[i, sub_neighborhood][0]), df.loc[i, sub_neighborhood][1]))\n",
    "    print(sub_neighborhood + ' DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018_3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['dorchester'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "\n",
    "for sub_neighborhood in articles.keys():\n",
    "    for (doc, article_id) in articles[sub_neighborhood]:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                name = ent[0:2]\n",
    "                sentence = ent.sent\n",
    "                people[sub_neighborhood].append((name, sentence, article_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_neighborhood in people.keys():\n",
    "    list1 = people[sub_neighborhood]\n",
    "    # insert the list to the set\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    people[sub_neighborhood] = unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_proportions = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "for sub_neighborhood in people.keys():\n",
    "    for i in range(len(people[sub_neighborhood])):\n",
    "        if people[sub_neighborhood][i][0].text.strip() != '':\n",
    "            temp = people[sub_neighborhood][i][0].text.split()\n",
    "            if len(temp) > 1:\n",
    "                people[sub_neighborhood][i] = (temp[-1], people[sub_neighborhood][i][1], people[sub_neighborhood][i][2])\n",
    "            else:\n",
    "                people[sub_neighborhood][i] = (temp[0], people[sub_neighborhood][i][1], people[sub_neighborhood][i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Melvin',\n",
       " Dr  Jim O’Connell  who leads Boston Health Care for the Homeless  was among those looking for Melvin  ,\n",
       " '2018_2901')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people['dorchester'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from ethnicolr import census_ln, pred_census_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = pd.DataFrame(people[white_neighborhoods[2]], columns=['last_name', 'article', 'article_id'])\n",
    "#temp1 = pd.DataFrame(people[white_neighborhoods[3]], columns=['last_name', 'article', 'article_id'])\n",
    "#pd.concat([pred_census_ln(temp, 'last_name', 2010), pred_census_ln(temp1, 'last_name', 2010)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1190: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2880: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "fenway DONE\n",
      "beacon_hill DONE\n",
      "downtown DONE\n",
      "south_boston DONE\n",
      "east_boston DONE\n",
      "back_bay DONE\n",
      "jamaica_plain DONE\n",
      "south_end DONE\n",
      "charlestown DONE\n",
      "brighton DONE\n",
      "allston DONE\n",
      "west_end DONE\n",
      "roslindale DONE\n",
      "north_end DONE\n",
      "mission_hill DONE\n",
      "harbor_islands DONE\n",
      "west_roxbury DONE\n",
      "dorchester DONE\n",
      "roxbury DONE\n",
      "mattapan DONE\n",
      "hyde_park DONE\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame(columns=['last_name', 'article', 'article_id'])\n",
    "sub_neighborhoods = white_neighborhoods + black_neighborhoods\n",
    "sub_neighborhoods.remove('longwood_medical_area')#\n",
    "for col in sub_neighborhoods:\n",
    "    temp = pd.DataFrame(people[col], columns=['last_name', 'article', 'article_id'])\n",
    "    preds = pred_census_ln(temp, 'last_name', 2010)\n",
    "    final_df = pd.concat([final_df, preds], axis=0)\n",
    "    print(col + ' DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('People_Covered_the_News/people_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# re-organize the data so that we have a way to retrieve original text\n",
    "# like adding ID to the dataset to identify each article\n",
    "# we should be able to find out the article a name comes from\n",
    "# we should also be able to find out which neighborhood an article talks about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# If name has 'word', 'word', then take the first name\n",
    "# keep sentence where name occurred, okay if multiple sentences\n",
    "# look at sentence where the name was mentioned \n",
    "# and the words which were used\n",
    "# end up with a dataset which has 'name' + 'sentence' + 'race'\n",
    "# try to put ID of article in the dataset as well, next to the sentence\n",
    "# for now, try to keep the row from which the name comes, or at least some form of ID\n",
    "\n",
    "# if extra time, group sentences by associated race\n",
    "# find most frequently used words for each race, maybe a word cloud or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "# come up with the population divide by races for each neighborhood\n",
    "# use neighborhood-separated articles\n",
    "# for each neighborhood, turn all articles into a spaCy-processible list of documents\n",
    "# for each list, extract out all people and run their last names with ethnicolr to predict races\n",
    "# for each set of predictions, get percentage of races\n",
    "# have journalism team go through U.S. Census data to see if the proportions of races match Census data\n",
    "\n",
    "# QUESTION: how to verify that two names talked about in an article belong to different people/the same people?\n",
    "\n",
    "# potential solution: for each article, only store the unique names; but is this possible? \n",
    "# each doc is an article, so we can extract out all \"PERSON\" entities and then keep only those which are unique\n",
    "# we could then feed the last names of those unique people (the last names may not necessarily be unique) to ethnicolr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
