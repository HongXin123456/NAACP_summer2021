{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np # linear algebra\r\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
    "import gensim\r\n",
    "from sklearn.manifold import TSNE\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "import random\r\n",
    "import gensim.models.doc2vec\r\n",
    "from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\r\n",
    "from gensim.models.doc2vec import Doc2Vec\r\n",
    "import numpy as np\r\n",
    "from os import path\r\n",
    "from PIL import Image\r\n",
    "from wordcloud import WordCloud, STOPWORDS \r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import networkx as nx\r\n",
    "from random import randint \r\n",
    "from itertools import count\r\n",
    "import networkx as nx\r\n",
    "import ast\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from collections import Counter\r\n",
    "from num2words import num2words\r\n",
    "import openpyxl\r\n",
    "import nltk\r\n",
    "import os\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "import copy\r\n",
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "import re\r\n",
    "import math\r\n",
    "import ast"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df = pd.read_csv('../Entity_Recognition/Neighborhood_Separated_Articles/2014.csv')\r\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "df = df.fillna(\"('no article', 'no_id')\")\r\n",
    "df['dorchester'] = df['dorchester'].apply(ast.literal_eval)\r\n",
    "df['roxbury'] = df['roxbury'].apply(ast.literal_eval)\r\n",
    "df['mattapan'] = df['mattapan'].apply(ast.literal_eval)\r\n",
    "df['hyde_park'] = df['hyde_park'].apply(ast.literal_eval)\r\n",
    "df['fenway'] = df['fenway'].apply(ast.literal_eval)\r\n",
    "df['beacon_hill'] = df['beacon_hill'].apply(ast.literal_eval)\r\n",
    "df['downtown'] = df['downtown'].apply(ast.literal_eval)\r\n",
    "df['south_boston'] = df['south_boston'].apply(ast.literal_eval)\r\n",
    "df['east_boston'] = df['east_boston'].apply(ast.literal_eval)\r\n",
    "df['back_bay'] = df['back_bay'].apply(ast.literal_eval)\r\n",
    "df['jamaica_plain'] = df['jamaica_plain'].apply(ast.literal_eval)\r\n",
    "df['south_end'] = df['south_end'].apply(ast.literal_eval)\r\n",
    "df['charlestown'] = df['charlestown'].apply(ast.literal_eval)\r\n",
    "df['brighton'] = df['brighton'].apply(ast.literal_eval)\r\n",
    "df['allston'] = df['allston'].apply(ast.literal_eval)\r\n",
    "df['west_end'] = df['west_end'].apply(ast.literal_eval)\r\n",
    "df['roslindale'] = df['roslindale'].apply(ast.literal_eval)\r\n",
    "df['north_end'] = df['north_end'].apply(ast.literal_eval)\r\n",
    "df['mission_hill'] = df['mission_hill'].apply(ast.literal_eval)\r\n",
    "df['harbor_islands'] = df['harbor_islands'].apply(ast.literal_eval)\r\n",
    "df['longwood_medical_area'] = df['longwood_medical_area'].apply(ast.literal_eval)\r\n",
    "df['west_roxbury'] = df['west_roxbury'].apply(ast.literal_eval)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "documents = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\r\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\r\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\r\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\r\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\r\n",
    "\r\n",
    "for col in df.columns:\r\n",
    "    tokens = []\r\n",
    "    for i in range(df.shape[0]):\r\n",
    "        article, _ = df.loc[i][col]\r\n",
    "        if article != 'no article':\r\n",
    "            text = word_tokenize(preprocess(article))\r\n",
    "            tokens = tokens + text\r\n",
    "    documents[col] = tokens\r\n",
    "    print(col)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-e817c64d313b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0marticle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'no article'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "788c7a583b49e0f108948e9844ef083ea97a54690da4d8e91da56fd70fa16a57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}